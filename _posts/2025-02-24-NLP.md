---
layout: post
title: "1. 绪论"
date:   2025-2-24
tags: [course, NLP]
comments: true
author: kioshiroi
---
# 1. 绪论
## 自然语言处理
    - 能够理解自然语言的意义（NLU）
    - 能够生成自然语言（NLG）

符号学派和随机学派

> ACL: CCF A
>### Sequence-to-Sequence 方法
> 自然语言处理中的S2S（Sequence-to-Sequence）方法是一种用于处理序列数据的深度学习技术，特别适用于自然语言处理任务，如机器翻译、文本摘要和对话生成。
> 1. **基本概念**：
>   - S2S方法由两个主要部分组成：编码器（Encoder）和解码器（Decoder）。
>   - 编码器将输入序列转换为一个固定长度的上下文向量。
>   - 解码器从该上下文向量生成输出序列。
> 2. **工作流程**：
>   - **编码阶段**：输入序列通过编码器，通常由RNN、LSTM或GRU组成，将信息压缩到上下文向量中。
>   - **解码阶段**：解码器接收上下文向量，并逐步生成输出序列。
> 3. **注意力机制**：
>  - 注意力机制允许解码器在生成每个输出时，动态地关注输入序列的不同部分。
> 4. **应用场景**：
>   - **机器翻译**、**文本摘要**、**对话生成**等。
> 5. **优势与挑战**：
>   - **优势**：处理变长输入和输出，适应性强。
>   - **挑战**：需要大量数据和计算资源，长序列处理具有挑战性。

## 自然语言处理的主要难点
- 语音歧义
- 词语切分歧义
- 语义歧义
- 语用歧义
- 结构歧义
- 指代歧义
- 省略歧义

人工反馈式强化学习RLHF

## 困惑度

困惑度（Perplexity）是衡量语言模型性能的一个重要指标，反映了模型对文本数据的预测能力。困惑度越低，表示模型对数据的预测越准确。

### 概念

- 困惑度是语言模型对测试集的平均不确定性的一种度量。
- 它是测试集上交叉熵的指数形式，表示模型在预测下一个词时的平均选择数。

### 计算过程

- 对于一个给定的语言模型，困惑度可以通过计算模型在测试集上的平均对数似然来得到。
- 公式为：
$$
  \text{Perplexity} = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i | w_1, w_2, ..., w_{i-1})}
$$
  其中，$ N $ 是测试集中词的总数，$ P(w_i | w_1, w_2, ..., w_{i-1}) $ 是模型预测词 $ w_i $ 的概率。


检索增强RAG
# 2. 词汇分析
> 不在课堂小测和作业的内容不会以计算形式出现

- 词是形式和意义相结合的单位，也是语言中能够独立运用的最小单位。
- 掌握一个词汇意味着知道其读音和语义
- 自然语言处理算法中**词**通常是基本单元
> llm中token是基础单位

- 词通常是由语素构成。
  - 语素是语言中意义的最小单元。
  - 简单词、复杂词
  - 实义词（表达具体的意义）、功能词（为了满足语法功能需求，也称闭类词）
- 词性（POS）也称词类
  - 名词
    - 专有名词是我们自然语言处理中需要特别关注的词汇。
  - 动词
    - 句子这一类分析重点关注的词汇。
  - 形容词
    - 情感分析重点关注的词汇。
  - 副词
    - 阅读理解比较关注。作用不大。
  - 数词
    - 自然语言中最难的一类词。
  - 代词
    - 代词消解->人称代词、物主代词
  - 冠词
    - 语法改错，没人做了，打不过大模型。
  - 介词
    - 句法分析。
  - 连词
    - but

- 词语规范化：将单词或词形转化为标准形式，针对由多种形式的单词使用一种单一的形式进行表示。

- 词形（Token）指的是在一个特定文档中的某个能够表达语义含义的字符序列。
- 词形还原：将一个词的多种形式还原为它们的基本形式。
- 词形分析：将一个词分解成为语素的过程。
- 词干提取

## 中文分词
  - 困难：
    - 分词规范
    - 歧义切分
      - 固有歧义
      - 真歧义
      - 交集歧义
    - 未登录词识别

### 基于最大匹配的中文分词
- 前向最大匹配
- 后向最大匹配

- 根据给定的字典，利用贪心搜索策略找到分词方案。

```python
def forward_max_match(sentence, dictionary, max_len=5):
    """
    前向最大匹配算法
    Args:
        sentence: 待分词的句子
        dictionary: 词典（set类型）
        max_len: 最大词长
    Returns:
        分词结果列表
    """
    words = []
    start = 0
    while start < len(sentence):
        word_len = min(max_len, len(sentence) - start)
        # 从最大长度开始尝试匹配
        while word_len > 0:
            word = sentence[start:start + word_len]
            if word in dictionary or word_len == 1:
                words.append(word)
                break
            word_len -= 1
        start += word_len
    return words

def backward_max_match(sentence, dictionary, max_len=5):
    """
    后向最大匹配算法
    Args:
        sentence: 待分词的句子
        dictionary: 词典（set类型）
        max_len: 最大词长
    Returns:
        分词结果列表
    """
    words = []
    end = len(sentence)
    while end > 0:
        word_len = min(max_len, end)
        # 从最大长度开始尝试匹配
        while word_len > 0:
            word = sentence[end - word_len:end]
            if word in dictionary or word_len == 1:
                words.insert(0, word)
                break
            word_len -= 1
        end -= word_len
    return words

# 示例使用
if __name__ == "__main__":
    # 示例词典
    dictionary = {"我们", "在", "学习", "自然", "语言", "处理", "自然语言", "自然语言处理"}
    
    # 测试句子
    sentence = "我们在学习自然语言处理"
    
    # 前向最大匹配
    forward_result = forward_max_match(sentence, dictionary)
    print("前向最大匹配结果:", "/".join(forward_result))
    
    # 后向最大匹配
    backward_result = backward_max_match(sentence, dictionary)
    print("后向最大匹配结果:", "/".join(backward_result))

```

### 基于线性链条件随机场的中文分词
- 将分词过程转化为**对字分类**的问题。
  - 对于输入橘子的每一个字，根据他在分词结果中的位置腹语不同的标签
  - BIES标注法：
    - 开始B，中间I，结尾E和单独成次S
> 他是研究生物科学的一位科学家
> 他|是|研究|生物科学|的|一位|科学家
> 他/S 是/S 研/B究/E 生/B物/I科/I学/E 的/S 一/B位/E 科/B学/I家/E
  - 条件随机场(不要求掌握)
    - 对于给定的观测序列，概率，势函数，马尔科夫状态转移
### BPE算法（Byte Pair Encoding）
- 基本原理：迭代地合并最频繁出现的字符对
- 具体步骤：
  1. 准备语料库，将所有单词拆分为字符序列，在每个单词末尾添加特殊符号
  2. 统计所有相邻字符对的频率
  3. 合并最频繁出现的字符对，形成新的子词
  4. 重复步骤2-3直到达到预设的词表大小或合并次数
- 优点：
  - 可以处理未登录词
  - 压缩效率高
  - 适应性强
- 示例：
  > "lower" -> "l o w e r </w>"
  > 多次合并后可能变成 "low er </w>"

### WordPiece算法
- 基本原理：基于概率最大化的子词分割算法
- 与BPE的主要区别：
  - 使用语言模型计算合并的收益
  - 选择合并后能最大化训练数据概率的字符对
- 具体步骤：
  1. 初始化词表为基本字符集
  2. 对每个可能的合并计算概率增益
  3. 选择增益最大的字符对合并
  4. 重复2-3直到达到预设条件
- 特点：
  - 常用词保持完整
  - 生僻词会被分解为更小的子词
  - 使用"##"标记词中间的子词
- 应用：
  - BERT等预训练模型的分词算法


### 中文分词的评价
- 评价指标
  - 准确率（Precision）：
    - 正确分词的词数 / 系统分词结果的总词数
    - P = 正确分词数 / 系统分词总数
  
  - 召回率（Recall）：
    - 正确分词的词数 / 标准答案的总词数
    - R = 正确分词数 / 标准答案总数
  
  - F1值：
    - 准确率和召回率的调和平均
    - F1 = 2 × P × R / (P + R)

- 评价难点
  - 分词标准不统一
  - 歧义切分的判定
  - 未登录词的处理

- 评价工具
  - SIGHAN分词评测工具
  - NLPIR分词评测系统

